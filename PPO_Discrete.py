# My first experience with PPO. The code is not intended to be imported into other programs
# and should be considered as a standalone program demonstrating the work of Proximal Policy Optimisation (PPO).
# The code is intended to be used by beginners like me to learn how PPO works.

# References:
# - https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L102
# - Also GPT-4o and 4o-mini were used (a very little).

# And also most of the explanatory comments were generated by GPT-4o from my sample, 'cause I'm fu*kin lazy schoolboy.

# Import libraryes
import torch
from torch import nn, tensor, optim, device, distributions
import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt

import gymnasium as gym

# Initializing environments
env = gym.make('CartPole-v1')                              # Create an instance of the CartPole environment
env_render = gym.make('CartPole-v1', render_mode='human')  # Create an environment for rendering

# Hyperparameters
EPISODES = 200  # Number of episodes to train the agent

GAMMA = 0.98        # Discount factor for future rewards
K_EPOCHS = 3        # Number of epochs to train the networks 
POLICY_CLIP = 0.15  # Clipping parameter for policy updates

ACTOR_RL = 0.0003   # Learning rate for the ActorNetwork 
CRITIC_RL = 0.0025  # Learning rate for the CriticNetwork

# Move computations to GPU if available
device = device('cuda' if torch.cuda.is_available() else 'cpu') 

# Create Memory class to store experiences
class Memory:
    def __init__(self):
        '''Initialize memory for: states, actions, rewards, done flags, state values, and log probabilities'''
        self.states = []    # Initialize states list
        self.actions = []   # Initialize actions list
        self.rewards = []   # Initialize rewards list
        self.dones = []     # Initialize done flags list
        self.values = []    # Initialize state values list
        self.log_probs = [] # Initialize log probabilities list

    def clear(self):
        '''Clear all accumulated data'''
        del self.states[:]  # Clear states
        del self.actions[:]  # Clear actions
        del self.rewards[:]  # Clear rewards
        del self.dones[:]    # Clear done flags
        del self.values[:]   # Clear state values
        del self.log_probs[:] # Clear log probabilities

# Define Actor network class
class ActorNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the model architecture as a sequential neural network
        self.model = nn.Sequential(
            nn.Linear(env.observation_space.shape[0], 128),   # Input layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, 128),                              # Hidden layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, env.action_space.n),               # Output layer (action probabilities)
            nn.Softmax(dim=-1)                                # Use Softmax for probability distribution
        )

        # Initialize weights using Xavier method and biases to zeros
        for layer in self.model:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)  # Initialize weights
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Initialize biases

        # Move the model to the selected device (GPU or CPU)
        self.to(device)
    
    def forward(self, state: tensor):
        pred = self.model(state)  # Make predictions using the model
        dist = distributions.Categorical(pred)  # Use Categorical distribution for action selection
        
        return dist  # Return the action distribution

# Define Critic network class
class CriticNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the model architecture as a sequential neural network
        self.model = nn.Sequential(
            nn.Linear(env.observation_space.shape[0], 128),  # Input layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, 128),                             # Hidden layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, 1)  # Single output neuron for state value estimation
        )
        
        # Initialize weights using Xavier method and biases to zeros
        for layer in self.model:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)  # Initialize weights
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Initialize biases
        
        # Move the model to the selected device (GPU or CPU)
        self.to(device)
    
    def forward(self, state: tensor):
        value = self.model(state)  # Compute the value of the state
        return value  # Return the state value

# Define the PPO class
class PPO:
    def __init__(self):
        '''Initialize basic attributes and hyperparameters for PPO'''
        self.Actor = ActorNetwork()  # Initialize the Actor network
        self.Critic = CriticNetwork()  # Initialize the Critic network

        self.Actor_old = ActorNetwork()  # Initialize the old version of the Actor network
        self.Actor_old.load_state_dict(self.Actor.state_dict())  # Load the current state into the old Actor

        self.loss_fn = nn.SmoothL1Loss()  # Define the loss function for regression

        # I chose SmoothL1Loss because, in my opinion, it handles regression tasks better and more stably.

        # Initialize the optimizer for both Actor and Critic networks
        self.optimizer = optim.AdamW([
            {'params': self.Actor.parameters(), 'lr': ACTOR_RL, 'amsgrad': True},
            {'params': self.Critic.parameters(), 'lr': CRITIC_RL, 'amsgrad': True}
        ])
    
    def action(self, state: tensor):
        '''Calculate action, value, and log probability.
           This function is intended only for the step() function and is not used in the training process'''
        
        with torch.no_grad():  # Disable gradient calculations for inference
            dist = self.Actor_old(state)  # Get the action distribution from the old Actor
        
        action = dist.sample()  # Sample an action from the distribution
        log_prob = dist.log_prob(action)  # Calculate the log probability of the chosen action
        value = self.Critic(state)  # Get the value of the current state

        return action.item(), value.item(), log_prob.item()  # Return scalar values for action, value, and log probability

    def gae_calculate(self, rewards, dones, values, next_value, gae_lambda=0.95):
        '''
        A small note: the method is implemented but is not used in the code.
        I struggled (for 2 days) to understand why my code didn't work,
        even though everything was calculated correctly. In the end, it started working
        after I detached this method from the training function, but I decided to keep it,
        as it may be useful to someone.
        '''
        gae = 0  # Initialize Generalized Advantage Estimation
        returns = []  # List to store computed returns

        # Iterate over rewards in reverse order
        for i in reversed(range(len(rewards))):
            # Calculate the TD error
            delta = rewards[i] + GAMMA * next_value * (1 - dones[i]) - values[i]
            # Update GAE
            gae = delta + GAMMA * gae_lambda * (1 - dones[i]) * gae

            # Insert computed return at the beginning of the list
            returns.insert(0, gae + values[i])

            next_value = values[i]  # Update next value for the next iteration

        return returns  # Return computed returns

    def education(self):
        '''Training process for the PPO algorithm'''

        total_reward = 0  # Total reward accumulated in the episode
        rewards = []  # List to store rewards

        # Traverse through rewards and done flags in reverse order
        for reward, done in zip(reversed(Memory.rewards), reversed(Memory.dones)):
            if done:  # Reset total reward if done flag is True
                total_reward = 0

            # Calculate discounted reward
            total_reward = reward + (GAMMA * total_reward)  
            rewards.insert(0, total_reward)  # Add the discounted reward to the beginning of the list
        
        rewards = tensor(rewards, dtype=torch.float32, device=device)  # Convert rewards list to a tensor
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)  # Normalize rewards

        # Convert remaining data to tensors
        old_states = tensor(np.array(Memory.states), dtype=torch.float32, device=device)
        old_actions = tensor(np.array(Memory.actions), dtype=torch.int64, device=device)
        old_log_probs = tensor(np.array(Memory.log_probs), dtype=torch.float32, device=device)
        old_values = tensor(np.array(Memory.values), dtype=torch.float32, device=device)

        # The following lines are commented out, as explained in the gae_calculate method.
        # values = self.Critic(old_states).squeeze(1)
        # next_value = values[-1].detach()
        # returns = self.gae_calculate(rewards, Memory.dones, values, next_value)
        # returns = tensor(returns, dtype=torch.float32, device=device)

        advantages = rewards.detach() - old_values.detach()  # Calculate advantages
        for _ in range(K_EPOCHS):  # Iterate over epochs
            dist = self.Actor(old_states)  # Get action distribution for old states
            log_probs = dist.log_prob(old_actions)  # Calculate log probabilities for old actions
            state_values = self.Critic(old_states).squeeze(1)  # Get state values for old states

            ratios = torch.exp(log_probs - old_log_probs.detach())  # Calculate probability ratios

            surr1 = ratios * advantages  # First surrogate objective
            surr2 = torch.clamp(ratios, 1 - POLICY_CLIP, 1 + POLICY_CLIP) * advantages  # Clipped surrogate objective
            
            # Calculate the total loss
            loss = -torch.min(surr1, surr2) + 0.5 * self.loss_fn(state_values, rewards) - 0.01 * dist.entropy()

            self.optimizer.zero_grad()  # Clear gradients
            loss.mean().backward()  # Backpropagate loss
            self.optimizer.step()  # Update network parameters

        self.Actor_old.load_state_dict(self.Actor.state_dict())  # Update the old Actor with current weights

        Memory.clear()  # Clear memory after training

# Define the Graphic class for plotting results
class Graphic:
    def __init__(self, x: str, y: str, title: str):
        plt.ion()  # Enable interactive mode for plotting

        self.fig, self.axis = plt.subplots()  # Create a new figure and axis for the plot

        self.axis.set_xlabel(x)  # Set x-axis label
        self.axis.set_ylabel(y)  # Set y-axis label
        self.axis.set_title(title)  # Set title of the plot

        self.accuracy_list = np.array([], dtype=np.float32)  # Initialize accuracy list
        self.episodes_list = np.array([], dtype=np.int32)  # Initialize episodes list
        
        self.line, = self.axis.plot([], [], color='blue')  # Create a line object for the plot

    def add(self, x: int, y: float):
        '''Add a new point to the plot'''
        self.accuracy_list = np.append(self.accuracy_list, y)  # Append new accuracy
        self.episodes_list = np.append(self.episodes_list, x)  # Append new episode number

        self.line.set_data(self.episodes_list, self.accuracy_list)  # Update line data
        self.axis.relim()  # Recompute the axis limits
        self.axis.autoscale_view()  # Autoscale the view

        plt.draw()  # Redraw the plot
        plt.pause(0.005)  # Pause briefly to update the plot

    def show(self, new_title: str):
        '''Show the final plot'''
        plt.ioff()  # Disable interactive mode

        self.axis.set_title(new_title)  # Set the title for the final plot

        plt.draw()  # Redraw the plot
        plt.show()  # Display the plot

PPO = PPO()  # Initialize the PPO algorithm

Memory = Memory()  # Initialize the memory object

Graphic = Graphic(x='Episodes',  # Create a Graphic instance with labels
                  y='Accuracy',
                  title='Graphic of Accuracy on episode',
                )

def step(episode: int, pbar: object):
    '''Execute one step in the environment for a given episode'''
    state = env.reset()[0]  # Reset the environment and get the initial state

    steps = 1  # Initialize step counter
    reward_per_episode = 0  # Initialize reward accumulator for the episode
    while True:

        state_tensor = tensor(state, dtype=torch.float32, device=device)  # Convert state to tensor
        action, value, log_prob = PPO.action(state_tensor)  # Get action, value, and log probability from PPO

        next_state, reward, done, truncated, _ = env.step(action)  # Execute action in the environment

        # Format state for progress bar display
        state_for_pbar = '.'.join(f'{x: .3f}' for x in state) 
        pbar.set_description(f"state: [{state_for_pbar}], action: {action}, reward: {reward}, done: {done or truncated}")

        # Store experience in memory
        Memory.states.append(state)  # Store current state
        Memory.actions.append(action)  # Store action taken
        Memory.rewards.append(reward)  # Store reward received
        Memory.dones.append(done)  # Store done flag
        Memory.values.append(value)  # Store value of the state
        Memory.log_probs.append(log_prob)  # Store log probability of action

        state = next_state  # Move to the next state

        reward_per_episode += reward  # Accumulate reward for the episode
        steps += 1  # Increment step counter

        if done or truncated:  # Break the loop if the episode is done
            break
    
    PPO.education()  # Train the PPO algorithm with the collected memory

    Graphic.add(episode, reward_per_episode)  # Add episode data to the plot

# Run the training loop
for episode in (pbar := tqdm(range(EPISODES))):
    step(episode + 1, pbar)  # Execute steps for each episode

    if (episode + 1) == (EPISODES - 10):  # Check if we are nearing the end of episodes
        env = env_render  # Switch to rendering mode for the last few episodes

Graphic.show('Final result')  # Show the final plot
