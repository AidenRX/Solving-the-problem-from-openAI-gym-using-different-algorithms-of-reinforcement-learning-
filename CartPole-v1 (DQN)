import torch
from torch import nn, optim, tensor
import numpy as np
import gym
import matplotlib.pyplot as plt
from random import sample, random
from collections import deque
from tqdm import tqdm

env = gym.make('CartPole-v1')
env_render = gym.make('CartPole-v1', render_mode='human')

EPISODES = 600

BATCH_SIZE = 128
TAU = 0.005

EPS = 1.0
EPS_MIN = 0.005
EPS_DECAY = EPS_MIN ** (1/EPISODES)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class ReplayBuffer:
    def __init__(self, buffer_size: int):
        self.memory = deque([], maxlen=buffer_size)
    
    def push(self, data: list):
        self.memory.append(data)
    
    def sample_batch(self, batch_size: int):
        return sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class Network(nn.Module):
    def __init__(self):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(env.observation_space.shape[0], 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, env.action_space.n)
        )

        for layer in self.model:
            if isinstance(layer, (nn.Conv2d, nn.Linear)):
                nn.init.xavier_uniform_(layer.weight)
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)
        
        self.to(device)
    
    def forward(self, X: tensor):
        return self.model(X)

class Agent:
    def __init__(self):
        self.Q_policy = Network()
        self.Q_target = Network()

        self.Q_target.load_state_dict(self.Q_policy.state_dict())

        self.loss_fn = nn.SmoothL1Loss()
        self.optimizer = optim.AdamW(params=self.Q_policy.parameters(), lr=0.001, amsgrad=True)

    def action(self, state: tensor):
        if EPS > random():
            action = env.action_space.sample()
            return action
        
        else:
            with torch.no_grad():
                action = np.array(self.Q_policy(state).argmax().item())
            return action
    
    def education(self):
        if len(Buffer) < BATCH_SIZE:
            return 

        batch = Buffer.sample_batch(BATCH_SIZE)

        obs, action, reward, next_obs, done = zip(*batch)

        obs, action, reward, next_obs, done = np.array(obs), np.array(action), np.array(reward), np.array(next_obs), np.array(done)
        obs, action, reward, next_obs, done = tensor(obs, dtype=torch.float32, device=device), tensor(action, dtype=torch.int64, device=device), tensor(reward, dtype=torch.float32, device=device), tensor(next_obs, dtype=torch.float32, device=device), tensor(done, dtype=torch.float32, device=device)

        self.Q_policy.train()

        with torch.no_grad():
            Q_of_next_states = self.Q_target(next_obs)

        Max_Q_values_next_states = Q_of_next_states.max(1)[0]

        Q_value = reward + (0.99 * Max_Q_values_next_states * 1 - done)
        Actions = self.Q_policy(obs).gather(1, action.unsqueeze(1))

        loss = self.loss_fn(Actions, Q_value.unsqueeze(1))

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.Q_policy.eval()
    
    def soft_update(self):
        target_net_state_dict = self.Q_target.state_dict()
        policy_net_state_dict = self.Q_policy.state_dict()

        for key in policy_net_state_dict:
            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
        self.Q_target.load_state_dict(target_net_state_dict)

def step(episode, pbar):
    global EPS

    obs, info = env.reset()

    total_reward = 0
    steps = 0

    done = False
    while not done:

        action = Agent.action(tensor(obs, dtype=torch.float32, device=device))

        next_obs, reward, done, truncated, _ = env.step(action)

        pbar.set_description(f"|| [{obs[0]: .4f}, {obs[1]: .4f}, {obs[2]: .4f}, {obs[3]: .4f}] || {action}, {EPS: .3f} || {reward} || {'True ' if done else 'False'} ||")

        Buffer.push([obs, action, reward, next_obs, done])

        obs = next_obs

        total_reward += reward
        steps += 1

        if (steps+1) % 10:
            Agent.education()

        if (steps+1) % 20:
            Agent.soft_update()
            
        if done or truncated:
            break
    
    EPS = max(EPS_MIN, EPS * EPS_DECAY)

    Graphic.add(episode, total_reward)

class Graphic:
    def __init__(self, x=str, y=str, title=str):
        plt.ion()

        plt.xlabel(x)
        plt.ylabel(y)
        plt.title(title)

        self.accuracy_list = []
        self.episodes_list = []

    def add(self, x, y):
        self.accuracy_list.append(y)
        self.episodes_list.append(x)

        plt.clf()
        plt.plot(self.episodes_list, self.accuracy_list)
        plt.pause(0.005)
    
    def show(self, new_title=str):
        plt.ioff()

        plt.title(new_title)

        plt.clf()
        plt.plot(self.episodes_list, self.accuracy_list)

        plt.show()

Graphic = Graphic(x='Episodes', y='Accuracy', title='Graphic of Accuracy on episode')

Buffer = ReplayBuffer(buffer_size=100000)

Agent = Agent()

for episode in (pbar := tqdm(range(EPISODES))):

    step(episode+1, pbar)

    if (episode+1) % (EPISODES-10) == 0:
        env = env_render

env.close()
Graphic.show(new_title='Result')
