import torch
from torch import nn, optim, tensor
import matplotlib.pyplot as plt
from collections import deque
import numpy as np
import gym
from random import sample
from tqdm import tqdm

# Create the environment for training
env = gym.make('Pendulum-v1')
env_render = gym.make('Pendulum-v1', render_mode='human')

# Define the dimensions of action and observation spaces
action_space = env.action_space.shape[0]
observ_space = env.observation_space.shape[0]

# Training parameters
EPISODES = 200
BATCH_SIZE = 128
MAXLEN = 100000
TAU = 0.005
GAMMA = 0.99

# Select the device for computation (GPU or CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class ReplayBuffer:
    def __init__(self, maxlen: int):
        self.memory = deque([], maxlen=maxlen)
    
    def push(self, data: list):
        # Add new experience to the buffer
        self.memory.append(data)

    def sample_batch(self, batch_size: int):
        # Sample a batch of data from the buffer
        return sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class ActorNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the actor network
        self.model = nn.Sequential(
            nn.Linear(observ_space, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_space),
            nn.Tanh()
        )
    
        self.to(device)

    def forward(self, obs: tensor):
        # Forward pass through the actor network
        pred = self.model(obs)
        return pred

class CriticNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the critic network
        self.model = nn.Sequential(
            nn.Linear(observ_space + action_space, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_space)
        )

        self.to(device)
    
    def forward(self, obs: tensor, action: tensor):
        # Concatenate observations and actions and forward pass through the critic network
        obs = torch.cat([obs, action], dim=1)
        pred = self.model(obs)
        return pred

class DDPG:
    def __init__(self):
        # Define optimizers and loss functions
        self.actor_optimizer = optim.AdamW(Actor.parameters(), lr=0.0005, amsgrad=True)
        self.critic_optimizer = optim.AdamW(Critic.parameters(), lr=0.005, amsgrad=True)
        self.loss_fn = nn.SmoothL1Loss()

        # Parameters for noise (OUNoise)
        self.mu = 0.0
        self.theta = 0.15
        self.sigma = 0.2
        self.current_noise = 0.0
    
    def action(self, obs: tensor):
        # Select an action with added noise
        with torch.no_grad():
            action = Actor(obs)
        noise = self.OUNoise()
        action += noise
        return np.clip(np.array([action.item()]), -2, 2)
    
    def OUNoise(self):
        # Generate noise using the Ornstein-Uhlenbeck process
        dx = self.theta * (self.mu - self.current_noise) + self.sigma * np.random.randn()
        self.current_noise += dx
        return self.current_noise
    
    def education(self):
        # Train the actor and critic networks
        if len(Buffer) < BATCH_SIZE:
            return 
        
        batch = Buffer.sample_batch(BATCH_SIZE)
        obs, action, reward, next_obs, done = zip(*batch)
        
        # Convert data to tensors
        obs = tensor(np.array(obs), dtype=torch.float32, device=device)
        action = tensor(np.array(action), dtype=torch.float32, device=device)
        reward = tensor(np.array(reward), dtype=torch.float32, device=device)
        next_obs = tensor(np.array(next_obs), dtype=torch.float32, device=device)
        done = tensor(np.array([1 - int(x) for x in done]), device=device)

        with torch.no_grad():
            next_action = Actor_target(next_obs)
            target_q_value = Critic_target(next_obs, next_action).squeeze(1)
            target_q_value = reward + GAMMA * target_q_value * done
        
        current_q_value = Critic(obs, action).squeeze(1)
        critic_loss = self.loss_fn(current_q_value, target_q_value)
        
        # Update critic network parameters
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Update actor network parameters
        predicted_action = Actor(obs)
        actor_loss = -Critic(obs, predicted_action).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
    
    def soft_update(self):
        # Soft update the target networks
        Actor_state_dict = Actor.state_dict()
        Critic_state_dict = Critic.state_dict()
        Actor_target_state_dict = Actor_target.state_dict()
        Critic_target_state_dict = Critic_target.state_dict()

        for key in Actor_state_dict:
            Actor_target_state_dict[key] = Actor_state_dict[key] * TAU + Actor_target_state_dict[key] * (1 - TAU)
        Actor_target.load_state_dict(Actor_target_state_dict)

        for key in Critic_state_dict:
            Critic_target_state_dict[key] = Critic_state_dict[key] * TAU + Critic_target_state_dict[key] * (1 - TAU)
        Critic_target.load_state_dict(Critic_target_state_dict)

class Graphic:
    def __init__(self, x: str, y: str, title: str):
        plt.ion()
        
        self.x = x
        self.y = y
        self.title = title

        self.episodes = []
        self.rewards = []

    def update(self, episode, reward):
        # Update the plot with new data
        self.episodes.append(episode)
        self.rewards.append(reward)

        plt.clf()
        plt.plot(self.episodes, self.rewards)
        plt.draw()
        plt.pause(0.05)

        plt.xlabel(self.x)
        plt.ylabel(self.y)
        plt.title(self.title)
    
    def show(self):
        # Show the final plot
        plt.ioff()
        
        plt.clf()
        plt.plot(self.episodes, self.rewards)
        plt.pause(0.05)

        plt.xlabel(self.x)
        plt.ylabel(self.y)
        plt.title(self.title)

        plt.show()

def step(pbar, episode):
    obs, info = env.reset()

    DDPG.current_noise = 0.0

    total_reward = 0
    truncated = False
    while True:
        action = DDPG.action(tensor(obs, dtype=torch.float32, device=device))

        next_obs, reward, done, truncated, _ = env.step(action)

        pbar.set_description(f"action: {action.item(): .4f} | reward: {reward: .4f}, {truncated}")

        Buffer.push([obs, action, reward, next_obs, truncated])

        if (episode+1) % 4 == 0:
            DDPG.education()

        if (episode+1) % 20 == 0:
            DDPG.soft_update()

        obs = next_obs
        total_reward += reward

        if truncated:
            break
    
    Graphic.update(episode, total_reward)

# Initialize ReplayBuffer and networks
Buffer = ReplayBuffer(MAXLEN)

Actor = ActorNetwork()
Actor_target = ActorNetwork()
Critic = CriticNetwork()
Critic_target = CriticNetwork()

DDPG = DDPG()

# Initialize the plotting
Graphic = Graphic(
    x='episode',
    y='reward',
    title='DDPG Pendulum-v1'
)

# Load the initial weights into target networks
Actor_target.load_state_dict(Actor.state_dict())
Critic_target.load_state_dict(Critic.state_dict())

# Main training loop
for episode in (pbar := tqdm(range(EPISODES))):
    step(pbar, episode)

    # Switch to rendering mode towards the end of training
    if (episode+1) == (EPISODES-10):
        env = env_render

env.close()

# Show the final plot
Graphic.show()
