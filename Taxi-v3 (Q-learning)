'''
Importing libraries: gym, numpy, matplotlib.pyplot, tqdm, and random subclasses.
'''
import gym
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

from random import random, randint

env = gym.make('Taxi-v3') # Creating the environment

plt.ion() # Enable interactive mode for plots

plt.figure(1) # Create a figure with number 1

# Edit the title and define the names for the x and y axes
plt.xlabel('Episodes')
plt.ylabel('Accuracy')
plt.title('In Progress...')

action_space = env.action_space.n # Number of discrete actions
observation_space = env.observation_space.n # Number of discrete states

# Create a table and fill it with random numbers between 0 and 0.4 for better environment exploration
Q_table = np.random.uniform(low=0, high=0.4, size=(observation_space, action_space))

episodes_list = [] # List of episodes
accuracy_list = [] # List of total rewards per episode

EPISODES = 6000 # Number of episodes

LR = 0.005 # Learning rate
GAMMA = 0.99 # Discount factor

EPS = 1.0 # Initial epsilon
EPS_MIN = 0.05 # Minimum epsilon
EPS_DECAY = EPS_MIN ** (1/EPISODES) # Formula for exponential epsilon decay

# Create a loop with pbar (i.e., progress bar)
for episode in (pbar := tqdm(range(EPISODES))):
    obs, info = env.reset() # Get the initial state of the environment

    total_reward = 0 # Total reward for the episode
    while True:
        # Action according to the e-greedy policy
        action = randint(0, action_space-1) if EPS > random() else np.argmax(Q_table[obs])

        next_obs, reward, done, _, _ = env.step(action) # Take a step

        # Update the progress bar
        pbar.set_description(f"obs: {obs} || action: {action}, eps: {EPS: .4f} || reward: {reward}, done: {done}")

        # Q-table update
        Q_table[obs, action] += LR * (reward + GAMMA * np.max(Q_table[next_obs]) - Q_table[obs, action])

        total_reward += reward # Add the reward to the total reward
        obs = next_obs # Update the state

        if done: # If the state is terminal
            episodes_list.append(episode) # Add the current episode to the list
            accuracy_list.append(total_reward) # Add the total reward (accuracy) to the list

            break # Exit the loop
    
    EPS = EPS * EPS_DECAY if EPS > EPS_MIN else EPS_MIN # Decrease epsilon

    plt.clf() # Clear the plot window
    plt.plot(episodes_list, accuracy_list) # Create a line plot
    plt.pause(0.001) # Pause

plt.ioff() # After the training loop ends, disable interactive mode

plt.title('Result') # Rename the title

# Clear the window and create a new plot
plt.clf()
plt.plot(episodes_list, accuracy_list)

# Show the final plot
plt.show()

env.close()
# Close the environment
