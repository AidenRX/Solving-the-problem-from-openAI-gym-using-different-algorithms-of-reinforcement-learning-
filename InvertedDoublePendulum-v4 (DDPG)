import torch
from torch import nn, optim, tensor
import matplotlib.pyplot as plt
from collections import deque
import numpy as np
from random import sample
from tqdm import tqdm
import gym

# Create the environment for the 'InvertedDoublePendulum-v4' task
env = gym.make('InvertedDoublePendulum-v4')
env_render = gym.make('InvertedDoublePendulum-v4', render_mode='human')  # Render environment for visualization

# Get action and observation dimensions
action_dim = env.action_space.shape[0]
observ_dim = env.observation_space.shape[0]

EPISODES = 200  # Number of training episodes

BATCH_SIZE = 128  # Mini-batch size for training
MAXLEN = 100000  # Maximum replay buffer size

TAU = 0.005  # Soft update factor for target networks
GAMMA = 0.99  # Discount factor for future rewards

# Set device to GPU if available, otherwise use CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class ReplayBuffer:
    """Replay buffer to store and sample experiences."""
    def __init__(self, maxlen: int):
        self.memory = deque([], maxlen=maxlen)  # Deque to store experiences
    
    def push(self, data: list):
        """Add a new experience to the buffer."""
        self.memory.append(data)

    def sample_batch(self, batch_size: int):
        """Sample a batch of experiences from the buffer."""
        return sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class ActorNetwork(nn.Module):
    """Actor network responsible for choosing actions."""
    def __init__(self, observ, action):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(observ, 128),  # Input layer with observation size
            nn.ReLU(),
            nn.Linear(128, 128),  # Hidden layer with 128 neurons
            nn.ReLU(),
            nn.Linear(128, action),  # Output layer with action size
            nn.Tanh()  # Use Tanh to ensure actions are in [-1, 1]
        )
    
        self.to(device)

    def forward(self, obs: tensor):
        """Forward pass through the actor network."""
        pred = self.model(obs)
        return pred

class CriticNetwork(nn.Module):
    """Critic network responsible for evaluating action-value pairs."""
    def __init__(self, observ, action):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(observ + action, 128),  # Input layer combining observation and action
            nn.ReLU(),
            nn.Linear(128, 128),  # Hidden layer with 128 neurons
            nn.ReLU(),
            nn.Linear(128, 1)  # Output layer predicting Q-value
        )

        self.to(device)
    
    def forward(self, obs: tensor, action: tensor):
        """Forward pass through the critic network. Concatenate observation and action."""
        obs = torch.cat([obs, action], dim=1)  # Combine observation and action as input
        pred = self.model(obs)
        return pred

class DDPG:
    """Deep Deterministic Policy Gradient (DDPG) agent."""
    def __init__(self):
        # Optimizers for actor and critic networks
        self.actor_optimizer = optim.AdamW(Actor.parameters(), lr=0.0005, amsgrad=True)
        self.critic_optimizer = optim.AdamW(Critic.parameters(), lr=0.005, amsgrad=True)

        self.loss_fn = nn.SmoothL1Loss()  # Huber loss for critic network

        # Ornstein-Uhlenbeck process parameters for exploration noise
        self.mu = 0.0
        self.theta = 0.15
        self.sigma = 0.2
        self.current_noise = 0.0
    
    def action(self, obs: tensor):
        """Select an action using the actor network and apply noise for exploration."""
        with torch.no_grad():
            action = Actor(obs)  # Get action from actor network
        noise = self.OUNoise()  # Add noise for exploration

        action += noise
        return np.clip(np.array([action.item()]), -2, 2)  # Clip action within valid range
    
    def OUNoise(self):
        """Generate noise using Ornstein-Uhlenbeck process."""
        dx = self.theta * (self.mu - self.current_noise) + self.sigma * np.random.randn()
        self.current_noise += dx
        return self.current_noise
    
    def education(self):
        """Train the agent using experiences from the replay buffer."""
        if len(Buffer) < BATCH_SIZE:
            return  # Return if the buffer doesn't have enough samples

        # Sample a batch of experiences
        batch = Buffer.sample_batch(BATCH_SIZE)
        obs, action, reward, next_obs, done = zip(*batch)
        
        # Convert to tensors
        obs = tensor(np.array(obs), dtype=torch.float32, device=device)
        action = tensor(np.array(action), dtype=torch.float32, device=device)
        reward = tensor(np.array(reward), dtype=torch.float32, device=device)
        next_obs = tensor(np.array(next_obs), dtype=torch.float32, device=device)
        
        done = tensor(np.array([1-int(x) for x in done]), device=device)  # Invert done flags

        # Compute target Q-value using target networks
        with torch.no_grad():
            next_action = Actor_target(next_obs)
            target_q_value = Critic_target(next_obs, next_action).squeeze(1)
            target_q_value = reward + GAMMA * target_q_value * done
        
        # Compute current Q-value using the critic network
        current_q_value = Critic(obs, action).squeeze(1)
        critic_loss = self.loss_fn(current_q_value, target_q_value)  # Huber loss
        
        # Update critic network
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Compute actor loss (policy gradient)
        predicted_action = Actor(obs)
        actor_loss = -Critic(obs, predicted_action).mean()  # Maximize Q-value
        
        # Update actor network
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
    
    def soft_update(self):
        """Perform a soft update of the target networks."""
        Actor_state_dict = Actor.state_dict()
        Critic_state_dict = Critic.state_dict()
        Actor_target_state_dict = Actor_target.state_dict()
        Critic_target_state_dict = Critic_target.state_dict()

        # Soft update for actor and critic target networks
        for key in Actor_state_dict:
            Actor_target_state_dict[key] = Actor_state_dict[key]*TAU + Actor_target_state_dict[key]*(1-TAU)
        Actor_target.load_state_dict(Actor_target_state_dict)

        for key in Critic_state_dict:
            Critic_target_state_dict[key] = Critic_state_dict[key]*TAU + Critic_target_state_dict[key]*(1-TAU)
        Critic_target.load_state_dict(Critic_target_state_dict)

class Graphic:
    """Class for handling real-time plotting of training progress."""
    def __init__(self, x: str, y: str, title: str):
        plt.ion()  # Enable interactive mode for real-time plotting
        
        self.x = x
        self.y = y
        self.title = title

        self.episodes = []
        self.rewards = []

    def update(self, episode, reward):
        """Update the plot with new episode and reward data."""
        self.episodes.append(episode)
        self.rewards.append(reward)

        plt.clf()
        plt.plot(self.episodes, self.rewards)
        plt.draw()
        plt.pause(0.05)

        plt.xlabel(self.x)
        plt.ylabel(self.y)
        plt.title(self.title)
    
    def show(self):
        """Display the final plot after training."""
        plt.ioff()
        
        plt.clf()
        plt.plot(self.episodes, self.rewards)
        plt.pause(0.05)

        plt.xlabel(self.x)
        plt.ylabel(self.y)
        plt.title(self.title)

        plt.show()

def step(pbar, episode):
    """Single step of agent training."""
    obs, info = env.reset()

    DDPG.current_noise = 0.0  # Reset noise for each episode

    total_reward = 0
    truncated = False
    while True:
        # Select an action using the DDPG agent
        action = DDPG.action(tensor(obs, dtype=torch.float32, device=device))

        next_obs, reward, done, truncated, _ = env.step(action)

        pbar.set_description(f"action: {action.item(): .4f} | reward: {reward: .4f}, done: {truncated}")

        # Store the experience in the replay buffer
        Buffer.push([obs, action, reward, next_obs, truncated])

        if (episode+1) % 4 == 0:
            DDPG.education()  # Train the agent every 4 episodes

        if (episode+1) % 20 == 0:
            DDPG.soft_update()  # Update the target networks every 20 episodes

        obs = next_obs
        total_reward += reward

        if truncated:
            break
    
    # Update the training progress plot
    Graphic.update(episode, total_reward)

# Initialize the replay buffer, networks, and DDPG agent
Buffer = ReplayBuffer(MAXLEN)

Actor = ActorNetwork(observ_dim, action_dim)
Actor_target = ActorNetwork(observ_dim, action_dim)
Critic = CriticNetwork(observ_dim, action_dim)
Critic_target = CriticNetwork(observ_dim, action_dim)

DDPG = DDPG()

Graphic = Graphic(
    x='episode',
    y='reward',
    title='DDPG Inverted Double Pendulum'
)

# Initialize target networks with the same weights as the main networks
Actor_target.load_state_dict(Actor.state_dict())
Critic_target.load_state_dict(Critic.state_dict())

# Main training loop
for episode in (pbar := tqdm(range(EPISODES))):
    step(pbar, episode)

    if (episode+1) == (EPISODES-10):
        env = env_render  # Switch to render mode in the last 10 episodes

# Close the environment and show the final training plot
env.close()
Graphic.show()
