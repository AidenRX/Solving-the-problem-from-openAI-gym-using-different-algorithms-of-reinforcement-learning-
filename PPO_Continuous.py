# My first experience with PPO. The code is not intended to be imported into other programs
# and should be considered as a standalone program demonstrating the work of Proximal Policy Optimisation (PPO).
# The code is intended to be used by beginners like me to learn how PPO works.

# References:
# - https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L102
# - Also GPT-4o and 4o-mini were used (a very little).

# And also most of the explanatory comments were generated by GPT-4o from my sample, 'cause I'm fu*kin lazy schoolboy.

'''Three important notes:

Important note first: In this implementation, you must configure the action scaling yourself 
in PPO.action(state: tensor): action = F.tanh(dist.sample()) * set_your_integer_own

Important note second: You must also configure the parameters of the reduction std 
that provides the agent's exploration of the environment yourself, based on your task.

Important note third: you can use either GAE training rewards or discounted_reward 
in this implementation. In my case for Pusher-v5, I decided to use GAE.
                                                                                                            '''
# Import libraries
import torch
from torch import nn, tensor, optim, device, distributions
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt

import gymnasium as gym

# Initialize environments
env = gym.make('Pusher-v5')                # Create an instance of the Pusher environment for training
env_render = gym.make('Pusher-v5', render_mode='human')  # Create an environment for rendering (visualization)

EPISODES = 200  # Number of episodes

ACTOR_LR = 0.0003
CRITIC_LR = 0.0025

ACTION_STD_INIT = 1.0  # Initialize standard deviation for action distribution

'''Important note second: You must also configure the parameters of the reduction std 
that provides the agent's exploration of the environment yourself, based on your task.'''

ACTION_STD = ACTION_STD_INIT
ACTION_STD_MIN = 0.005  # Minimum action standard deviation
ACTION_STD_DECAY = ACTION_STD_MIN ** (1/EPISODES)  # exponential decay factor for action standard deviation

GAMMA = 0.98  # Discount factor for future rewards
POLICY_CLIP = 0.2  # Clipping parameter for policy updates
K_EPOCHS = 7  # Number of epochs for training

device = device('cuda' if torch.cuda.is_available() else 'cpu')  # Choose GPU or CPU

# Create Memory class to store experiences
class Memory:
    def __init__(self):
        '''Initialize memory for: states, actions, rewards, done flags, state values, and log probabilities'''
        self.states = []    # Initialize states list
        self.actions = []   # Initialize actions list
        self.rewards = []   # Initialize rewards list
        self.dones = []     # Initialize done flags list
        self.values = []    # Initialize state values list
        self.log_probs = [] # Initialize log probabilities list

    def clear(self):
        '''Clear all accumulated data'''
        del self.states[:]  # Clear states
        del self.actions[:]  # Clear actions
        del self.rewards[:]  # Clear rewards
        del self.dones[:]    # Clear done flags
        del self.values[:]   # Clear state values
        del self.log_probs[:] # Clear log probabilities

# Define Actor-Critic network class
class ActorCritic(nn.Module):
    def __init__(self):
        super().__init__()

        self.action_var = torch.full(size=(env.action_space.shape[0],), fill_value=ACTION_STD_INIT ** 2, device=device)  # Action variance

        self.Actor = nn.Sequential(
            nn.Linear(env.observation_space.shape[0], 128),   # Input layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, 128),                              # Hidden layer
            nn.ReLU(),                                        # Activation function
            nn.Linear(128, env.action_space.shape[0])         # Output layer (action probabilities)
        )

        self.Critic = nn.Sequential(
            nn.Linear(env.observation_space.shape[0], 128), # Input layer
            nn.ReLU(),                                      # Activation function
            nn.Linear(128, 128),                            # Hidden layer
            nn.ReLU(),                                      # Activation function
            nn.Linear(128, 1)                               # Output layer (value of state)
        )

        # Xavier uniform initialization of layer weights for Actor
        for layer in self.Actor:
            if isinstance(layer, (nn.Conv2d, nn.Linear)):
                nn.init.xavier_uniform_(layer.weight)  # Initialize weights
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Initialize biases

        # Xavier uniform initialization of layer weights for Critic
        for layer in self.Critic:
            if isinstance(layer, (nn.Conv2d, nn.Linear)):
                nn.init.xavier_uniform_(layer.weight)  # Initialize weights
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Initialize biases

        self.to(device)  # Move to device (GPU or CPU)

    def forward(self, state: tensor):  # Forward method is not used
        raise NotImplementedError

    def get_dist(self, state: tensor):
        mu = self.Actor(state)  # Get mean action from the Actor
        cov_mat = torch.diag(self.action_var)  # Create covariance matrix for action distribution

        dist = distributions.MultivariateNormal(mu, cov_mat)  # Define a multivariate normal distribution

        return dist  # Return the action distribution
    
    def get_value(self, state: tensor):
        return self.Critic(state)  # Get the value of the state from the Critic
    
    def evaluate(self, state: tensor, action: tensor):
        dist = self.get_dist(state)  # Get action distribution

        log_probs = dist.log_prob(action.unsqueeze(1)).sum(dim=-1)  # Calculate log probabilities of actions
        value = self.Critic(state).squeeze(1)  # Get state value
        dist_entropy = dist.entropy()  # Calculate distribution entropy

        return log_probs, value, dist_entropy  # Return log probabilities, value, and entropy

class PPO:
    def __init__(self):
        self.policy = ActorCritic()  # Initialize Actor-Critic policy

        self.policy_old = ActorCritic()  # Old policy for computing updates
        self.policy_old.load_state_dict(self.policy.state_dict())  # Load initial weights

        self.loss_fn = nn.SmoothL1Loss()  # Loss function for training
        self.optimizer = optim.AdamW([  # Adam optimizer with weight decay
            {'params': self.policy.Actor.parameters(), 'lr': ACTOR_LR, 'amsgrad': True},
            {'params': self.policy.Critic.parameters(), 'lr': CRITIC_LR, 'amsgrad': True}
        ])

    def action(self, state: tensor):
        with torch.no_grad():  # Disable gradient calculation
            dist = self.policy_old.get_dist(state)  # Get action distribution from old policy

        '''Important note first: In this implementation, you must configure the action scaling yourself 
        in PPO.action(state: tensor): action = F.tanh(dist.sample()) * set_your_integer_own'''

        action = F.tanh(dist.sample()) * 2  # Sample action and scale it
        value = self.policy.get_value(state)  # Get value of the state
        log_probs = dist.log_prob(action)  # Calculate log probabilities of actions

        return [a.item() for a in action], value.item(), log_probs.item()  # Return action, value, and log probability

    def compute_gae(self, rewards, dones, values, next_value, 
                    GAE_lambda=0.95):
        
        gae = 0  # Initialize Generalized Advantage Estimation
        returns = []  # List to hold returns
        for step in reversed(range(len(rewards))):  # Iterate backwards through rewards
            delta = rewards[step] + GAMMA * next_value * (1 - dones[step]) - values[step]  # Compute TD error
            gae = delta + GAMMA * GAE_lambda * (1 - dones[step]) * gae  # Update GAE
            returns.insert(0, gae + values[step])  # Store returns
            next_value = values[step]  # Update next value for the next iteration

        return returns  # Return computed returns

    def compute_discounted_reward(self, rewards: list, dones: list):
        total_reward = 0  # Initialize total reward
        rewards = []  # List to hold rewards
        for reward, done in zip(reversed(rewards), reversed(dones)):  # Iterate backwards through rewards and dones
            if done:
                total_reward = 0  # Reset total reward if episode is done

            total_reward = reward + (GAMMA * total_reward)  # Update total reward

            rewards.insert(0, total_reward)  # Store reward
        
        rewards = tensor(rewards, dtype=torch.float32, device=device)  # Convert rewards to tensor
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)  # Normalize rewards

        return rewards  # Return normalized rewards

    def education(self):
        # Convert memory data to tensors
        old_states = tensor(np.array(Memory.states), dtype=torch.float32, device=device)
        old_actions = tensor(np.array(Memory.actions), dtype=torch.float32, device=device)
        old_log_probs = tensor(np.array(Memory.log_probs), dtype=torch.float32, device=device)
        old_values = tensor(np.array(Memory.values), dtype=torch.float32, device=device)

        '''Important note third: you can use either GAE training rewards or discounted_reward 
        in this implementation. In my case for Pusher-v5, I decided to use GAE.'''

        state_values = self.policy.get_value(old_states).squeeze(1)  # Get state values
        next_value = state_values[-1].detach()  # Get the last state value
        returns = self.compute_gae(Memory.rewards, Memory.dones, state_values, next_value)  # Compute GAE returns
        returns = tensor(returns, dtype=torch.float32, device=device)  # Convert returns to tensor

        advantages = returns.detach() - old_values.detach()  # Calculate advantages
        for _ in range(K_EPOCHS):  # Update policy for K epochs
            log_probs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)  # Evaluate old states and actions

            ratios = torch.exp(log_probs - old_log_probs.detach())  # Calculate policy ratios

            surr1 = ratios * advantages  # First surrogate objective
            surr2 = torch.clamp(ratios, 1 - POLICY_CLIP, 1 + POLICY_CLIP) * advantages  # Clipped surrogate objective
            
            loss = -torch.min(surr1, surr2) + 0.5 * self.loss_fn(state_values, returns) - 0.01 * dist_entropy  # Total loss

            self.optimizer.zero_grad()  # Zero the gradients
            loss.mean().backward()  # Backpropagation
            self.optimizer.step()  # Update parameters

        global ACTION_STD

        ACTION_STD = max(ACTION_STD * ACTION_STD_DECAY, ACTION_STD_MIN)  # Decay action standard deviation

        new_action_var = torch.full(size=(env.action_space.shape[0],), fill_value=ACTION_STD ** 2, device=device)  # Update action variance

        self.policy.action_var = new_action_var  # Set new action variance
        self.policy_old.action_var = new_action_var  # Set old policy action variance

        self.policy_old.load_state_dict(self.policy.state_dict())  # Update old policy weights
        
        Memory.clear()  # Clear memory

class Graphic:
    def __init__(self, x: str, y: str, title: str):
        plt.ion()  # Enable interactive mode

        self.fig, self.axis = plt.subplots()  # Create a figure and axis for plotting

        self.axis.set_xlabel(x)  # Set x-axis label
        self.axis.set_ylabel(y)  # Set y-axis label
        self.axis.set_title(title)  # Set plot title

        self.episodes = np.array([], dtype=np.int64)  # Initialize episodes array
        self.rewards = np.array([], dtype=np.float32)  # Initialize rewards array

        self.line, = self.axis.plot([], [], color='blue')  # Initialize line for plotting
    
    def update(self, episode: int, reward: float):
        self.episodes = np.append(self.episodes, episode)  # Append new episode
        self.rewards = np.append(self.rewards, reward)  # Append new reward

        self.line.set_data(self.episodes, self.rewards)  # Update line data
        self.axis.relim()  # Recalculate limits
        self.axis.autoscale_view()  # Rescale view

        plt.draw()  # Update the plot
        plt.pause(0.005)  # Pause for plot refresh

    def show(self):
        plt.ioff()  # Disable interactive mode

        self.line.set_data(self.episodes, self.rewards)  # Update line data for final display
        self.axis.relim()  # Recalculate limits
        self.axis.autoscale_view()  # Rescale view

        plt.draw()  # Update the plot
        plt.pause(0.005)  # Pause for plot refresh

        plt.show()  # Show the final plot

PPO = PPO()  # Initialize PPO instance
Memory = Memory()  # Initialize Memory instance
Graphic = Graphic(  # Initialize Graphic instance for plotting
    x='episodes',
    y='rewards',
    title='Graphic of rewards per episode'
)

def step(episode: int, pbar: object):
    state = env.reset()[0]  # Reset environment and get initial state

    reward_per_episode = 0  # Initialize reward for the episode
    while True:
        state_tensor = tensor(state, dtype=torch.float32, device=device)  # Convert state to tensor
        
        action, value, log_prob = PPO.action(state_tensor)  # Get action from PPO

        next_state, reward, done, truncated, _ = env.step(action)  # Step in the environment with the action

        #state_for_pbar = '.'.join(f"{x: .1f}" for x in state)
        action_for_pbar = '.'.join(f'{x: .1f}' for x in action)  # Format action for progress bar display
        pbar.set_description(f"action: [{action_for_pbar}], std: {ACTION_STD: .3f}, reward: {reward: .3f}, done: {done or truncated}")  # Update progress bar description

        Memory.states.append(state)  # Store state
        Memory.actions.append(action)  # Store action
        Memory.rewards.append(reward)  # Store reward
        Memory.dones.append(done)  # Store done flag
        Memory.values.append(value)  # Store state value
        Memory.log_probs.append(log_prob)  # Store log probability

        state = next_state  # Update state
        reward_per_episode += reward  # Accumulate reward for the episode

        if done or truncated:  # Check if episode is done
            break
    
    PPO.education()  # Update the policy based on experiences

    Graphic.update(episode, reward_per_episode)  # Update the graphic with new data

for episode in (pbar := tqdm(range(EPISODES))):  # Iterate over episodes with a progress bar
    step(episode + 1, pbar)  # Step through the environment

    if (episode + 1) == (EPISODES - 10):
        env = env_render  # Visualizing last episodes

Graphic.show()  # Show the final plot
