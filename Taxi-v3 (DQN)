import torch
from torch import nn, optim, tensor

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

from random import random, sample
from collections import deque
from tqdm import tqdm

env = gym.make('Taxi-v3')

EPISODES = 600  # Number of training episodes
MAX_LEN = 100000  # Maximum size of the replay buffer
BATCH_SIZE = 128  # Batch size for training
TAU = 0.0005  # Soft update coefficient
GAMMA = 0.99  # Discount factor
LR = 0.005  # Learning rate

EPS = 1.0  # Initial exploration rate (epsilon)
EPS_MIN = 0.005  # Minimum exploration rate
EPS_DECAY = EPS_MIN ** (1/EPISODES)  # Epsilon decay rate

# Select device: GPU (CUDA) if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)

# Replay buffer to store experiences
class ReplayBuffer:
    def __init__(self, maxlen: int):
        self.memory = deque([], maxlen=maxlen)  # Use deque to store experiences
    
    # Add a new experience to the buffer
    def push(self, data: list):
        self.memory.append(data)
    
    # Sample a batch of experiences from the buffer
    def sample_data(self, batch_size: int):
        return sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

# Neural network for Q-learning
class Network(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the network architecture
        self.model = nn.Sequential(
            nn.Linear(1, 128),  # Input layer
            nn.ReLU(),  # Activation function
            nn.Linear(128, 128),  # Hidden layer
            nn.ReLU(),  # Activation function
            nn.Linear(128, 6),  # Output layer (6 actions for Taxi-v3)
        )
        
        # Initialize network weights
        for layer in self.model:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)  # Xavier initialization
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Bias initialized to 0

        self.to(device)  # Move model to selected device

    def forward(self, X: tensor):
        pred = self.model(X)  # Forward pass through the network
        return pred

# Agent class for interacting with the environment
class Agent:
    def __init__(self):
        self.Q_policy = Network()  # Policy network
        self.Q_target = Network()  # Target network

        self.Q_target.load_state_dict(self.Q_policy.state_dict())  # Copy weights

        # Define loss function and optimizer
        self.loss_fn = nn.SmoothL1Loss()
        self.optimizer = optim.AdamW(self.Q_policy.parameters(), lr=LR, amsgrad=True)

    # Select an action using epsilon-greedy policy
    def Action(self, obs: tensor):
        if EPS > random():  # Exploration
            action = env.action_space.sample()  # Random action
            return action
        else:  # Exploitation
            with torch.no_grad():
                action = self.Q_policy(obs).argmax().item()  # Select action with highest Q-value
            return action
    
    # Train the Q-network
    def Education(self):
        if len(Buffer) < BATCH_SIZE:
            return  # Skip if buffer is not filled enough

        self.Q_policy.train()  # Set policy network to training mode
        
        batch = Buffer.sample_data(BATCH_SIZE)  # Sample batch from replay buffer

        # Convert batch to tensors and move to device
        obs, action, reward, next_obs, done = zip(*batch)
        obs, action, reward, next_obs, done = np.array(obs), np.array(action), np.array(reward), np.array(next_obs), np.array(done)
        obs, action, reward, next_obs, done = tensor(obs, dtype=torch.float32, device=device), tensor(action, dtype=torch.int64, device=device), tensor(reward, device=device), tensor(next_obs, dtype=torch.float32, device=device), tensor(done, dtype=torch.int64, device=device)

        with torch.no_grad():
            Q_value_of_next_states = self.Q_target(next_obs)  # Compute Q-values for next states
        Max_Q_values = Q_value_of_next_states.max(1)[0]  # Take max Q-value for each next state
        
        # Compute the target Q-value
        Target_Q_value = reward + GAMMA * Max_Q_values * (1 - done)
        Q_value = self.Q_policy(obs).gather(1, action.unsqueeze(1)).squeeze(1)  # Get Q-value for current state-action pairs

        loss = self.loss_fn(Q_value, Target_Q_value)  # Compute loss

        self.optimizer.zero_grad()
        loss.backward()  # Backpropagate loss
        nn.utils.clip_grad_value_(self.Q_policy.parameters(), 10)  # Gradient clipping
        self.optimizer.step()  # Update weights

        self.Q_policy.eval()  # Set policy network to evaluation mode
    
    # Perform soft update of the target network
    def soft_update(self):
        policy_state_dict = self.Q_policy.state_dict()
        target_state_dict = self.Q_target.state_dict()

        # Update target network parameters
        for key in policy_state_dict:
            target_state_dict[key] = policy_state_dict[key] * TAU + target_state_dict[key] * (1 - TAU)
        self.Q_target.load_state_dict(target_state_dict)

# Class for plotting progress during training
class Graphic:
    def __init__(self, x: str, y: str, title: str):
        plt.ion()  # Interactive mode for live updates

        plt.figure(1)
        plt.xlabel(x)
        plt.ylabel(y)
        plt.title(title)

        self.list_of_episodes = []  # Store episodes
        self.list_of_accuracy = []  # Store accuracy (rewards)

    # Update plot with new data
    def update(self, episode: int, accuracy: int):
        self.list_of_episodes.append(episode)
        self.list_of_accuracy.append(accuracy)

        plt.clf()
        plt.plot(self.list_of_episodes, self.list_of_accuracy)
        plt.draw()
        plt.pause(0.001)  # Pause for live update
    
    # Display final plot
    def show(self, new_title: int):
        plt.ioff()  # Turn off interactive mode

        plt.title(new_title)

        plt.clf()
        plt.plot(self.list_of_episodes, self.list_of_accuracy)

        plt.show()

# Function for performing a single episode step
def step(pbar):
    global EPS

    obs, info = env.reset()  # Reset the environment and get initial state

    reward_per_episode = 0  # Total reward for the episode
    steps = 0

    while True:
        action = Agent.Action(tensor([obs], dtype=torch.float32, device=device))  # Select an action

        # Perform the action and observe the next state and reward
        next_obs, reward, done, truncated, _ = env.step(action)

        pbar.set_description(f"Obs: {obs} || Action: {action}, EPS: {EPS: .3f} || reward: {reward}, done: {done}")

        # Store the experience in the replay buffer
        Buffer.push([[obs], action, reward, [next_obs], done])

        if (steps + 1) % 5 == 0:
            Agent.Education()  # Train the network every 5 steps

        if (steps + 1) % 200 == 0:
            Agent.soft_update()  # Update target network every 200 steps

        obs = next_obs  # Move to the next state

        reward_per_episode += reward
        steps += 1

        if done or truncated:  # End the episode if done
            break

    EPS = max(EPS_MIN, EPS * EPS_DECAY)  # Decay epsilon

    Graphic.update(episode, reward_per_episode)  # Update plot with episode reward

# Initialize replay buffer
Buffer = ReplayBuffer(MAX_LEN)

# Initialize agent
Agent = Agent()

# Initialize plot for training progress
Graphic = Graphic(
    x='episodes',
    y='accuracy',
    title='In progress...'
)

# Main training loop
for episode in (pbar := tqdm(range(EPISODES))):
    step(pbar)

env.close()

# Show final plot
Graphic.show(new_title='Result')
