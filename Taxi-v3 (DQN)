import torch
from torch import nn, optim, tensor
import gym
import numpy as np
import matplotlib.pyplot as plt
from random import random, sample
from collections import deque
from tqdm import tqdm

# Create a Gym environment for the Taxi task
env = gym.make('Taxi-v3')

# Training parameters and hyperparameters
EPISODES = 600             # Number of training episodes
MAX_LEN = 100000           # Maximum length of the replay buffer
BATCH_SIZE = 128           # Mini-batch size for training
TAU = 0.0005               # Coefficient for soft updating the target network
GAMMA = 0.99               # Discount factor
LR = 0.005                 # Learning rate

# Parameters for the ε-greedy policy
EPS = 1.0                  # Initial value of ε
EPS_MIN = 0.005            # Minimum value of ε
EPS_DECAY = EPS_MIN ** (1/EPISODES)  # Decay rate for ε

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available

print(device)

class ReplayBuffer:
    """Class for the experience replay buffer."""
    def __init__(self, maxlen: int):
        self.memory = deque([], maxlen=maxlen)
    
    def push(self, data: list):
        """Add new experience to the buffer."""
        self.memory.append(data)
    
    def sample_data(self, batch_size: int):
        """Randomly sample a mini-batch of data from the buffer."""
        return sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class Network(nn.Module):
    """Neural network for training the agent."""
    def __init__(self):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(1, 128),    # Input layer with 1 neuron
            nn.ReLU(),            # ReLU activation function
            nn.Linear(128, 128),  # Hidden layer with 128 neurons
            nn.ReLU(),            # ReLU activation function
            nn.Linear(128, 6),    # Output layer with 6 neurons (number of actions)
        )
        
        self.loss_fn = nn.SmoothL1Loss()  # Loss function
        self.optimizer = optim.AdamW(self.parameters(), lr=LR, amsgrad=True)  # Optimizer

        self.to(device)

    def forward(self, X: tensor):
        """Forward pass through the network."""
        pred = self.model(X)
        return pred

class Agent(Network):
    """Agent that uses the neural network for action selection and training."""
    def __init__(self):
        super().__init__()

    def Action(self, obs: tensor):
        """Select an action based on the ε-greedy policy."""
        if EPS > random():
            # Random action
            action = env.action_space.sample()
        else:
            # Action that maximizes Q-value
            with torch.no_grad():
                action = self.forward(obs).argmax().item()
        return action
    
    def Education(self, Buffer, Q_target):
        """Train the network using data from the replay buffer."""
        if len(Buffer) < BATCH_SIZE:
            return 
        
        self.train()  # Switch to training mode
        
        batch = Buffer.sample_data(BATCH_SIZE)

        obs, action, reward, next_obs = zip(*batch)

        # Convert data to tensors
        obs = tensor(np.array(obs), dtype=torch.float32, device=device)
        action = tensor(np.array(action), dtype=torch.int64, device=device)
        reward = tensor(np.array(reward), device=device)
        next_obs = tensor(np.array(next_obs), dtype=torch.float32, device=device)

        # Compute target Q-values
        with torch.no_grad():
            Q_value_of_next_states = Q_target(next_obs)
        Max_Q_values = Q_value_of_next_states.max(1)[0]
        Target_Q_value = reward + GAMMA * Max_Q_values

        # Compute current Q-value and loss function
        Q_value = self.forward(obs).gather(1, action.unsqueeze(1)).squeeze(1)
        loss = self.loss_fn(Q_value, Target_Q_value)

        # Update network parameters
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_value_(self.parameters(), 10)  # Clip gradients
        self.optimizer.step()

        self.eval()  # Switch to evaluation mode
    
    def soft_update(self, Q_target, Q_policy):
        """Soft update of the target network."""
        policy_state_dict = Q_policy.state_dict()
        target_state_dict = Q_target.state_dict()

        for key in policy_state_dict:
            target_state_dict[key] = policy_state_dict[key] * TAU + target_state_dict[key] * (1 - TAU)
        Q_target.load_state_dict(target_state_dict)

    def Reset_parameters(self):
        """Reset the network parameters."""
        for param in self.model.children():
            if hasattr(param, 'reset_parameters'):
                param.reset_parameters()

class Graphic:
    """Class for plotting graphs."""
    def __init__(self, x: str, y: str, title: str):
        plt.ion()  # Enable interactive mode

        plt.figure(1)
        plt.xlabel(x)
        plt.ylabel(y)
        plt.title(title)

        self.list_of_episodes = []
        self.list_of_accuracy = []

    def update(self, episode: int, accuracy: int):
        """Update the graph."""
        self.list_of_episodes.append(episode)
        self.list_of_accuracy.append(accuracy)
        plt.clf()  # Clear the current plot
        plt.plot(self.list_of_episodes, self.list_of_accuracy)
        plt.pause(0.001)  # Pause to refresh the graph
    
    def show(self, new_title: int):
        """Show the final graph."""
        plt.ioff()  # Disable interactive mode
        plt.title(new_title)
        plt.clf()
        plt.plot(self.list_of_episodes, self.list_of_accuracy)
        plt.show()

def step(pbar):
    """Agent training step."""
    global EPS

    obs, info = env.reset()

    reward_per_episode = 0

    steps = 0
    while steps != 200:
        action = Agent.Action(tensor([obs], dtype=torch.float32, device=device))

        next_obs, reward, done, _, _ = env.step(action)

        #env.render()  # Optional: for visualizing agent actions in the environment

        pbar.set_description(f"Obs: {obs} || Action: {action}, EPS: {EPS: .3f} || reward: {reward}, done: {done}")

        Buffer.push([[obs], action, reward, [next_obs]])

        # Train and update the network
        if (steps + 1) % 5 == 0:
            Agent.Education(Buffer, Q_target)
        
        if (steps + 1) % 100 == 0:
            Agent.soft_update(Q_target, Q_policy)

        obs = next_obs

        reward_per_episode += reward
        steps += 1

        if done:
            break

    # Update ε value
    EPS = EPS * EPS_DECAY if EPS > EPS_MIN else EPS_MIN

    Graphic.update(episode, reward_per_episode)

# Initialize the buffer, network, and agent
Buffer = ReplayBuffer(MAX_LEN)
Q_policy = Network()
Q_target = Network()
Q_target.load_state_dict(Q_policy.state_dict())
Agent = Agent()
Graphic = Graphic(x='episodes', y='accuracy', title='In progress...')

# Main training loop
for episode in (pbar := tqdm(range(EPISODES))):
    step(pbar)

# Close environment and show the graph
env.close()
Graphic.show(new_title='Result')
